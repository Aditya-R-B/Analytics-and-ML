{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"8ArWK463kbhL","colab":{"base_uri":"https://localhost:8080/","height":17},"outputId":"ea3b25c7-4050-4ecd-c5aa-9c146b096876","executionInfo":{"status":"ok","timestamp":1661869620032,"user_tz":-330,"elapsed":5520,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}}},"source":["import numpy as np\n","import pandas as pd\n","import plotly\n","import plotly.figure_factory as ff\n","import plotly.graph_objs as go\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n","init_notebook_mode(connected=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["        <script type=\"text/javascript\">\n","        window.PlotlyConfig = {MathJaxConfig: 'local'};\n","        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n","        if (typeof require !== 'undefined') {\n","        require.undef(\"plotly\");\n","        requirejs.config({\n","            paths: {\n","                'plotly': ['https://cdn.plot.ly/plotly-2.8.3.min']\n","            }\n","        });\n","        require(['plotly'], function(Plotly) {\n","            window._Plotly = Plotly;\n","        });\n","        }\n","        </script>\n","        "]},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uas98UgZJgqV","executionInfo":{"status":"ok","timestamp":1661870256739,"user_tz":-330,"elapsed":3245,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}},"outputId":"1104cbf2-003f-414b-fad5-e3a64606a47b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"5mldzJdakbhS"},"source":["data = pd.read_csv('/task_b.csv')\n","data=data.iloc[:,1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"rsCrC2wckbhV","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"552fdf34-0b24-4b30-ba61-51ecca553182","executionInfo":{"status":"ok","timestamp":1661870579220,"user_tz":-330,"elapsed":608,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}}},"source":["data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["            f1            f2        f3    y\n","0  -195.871045 -14843.084171  5.532140  1.0\n","1 -1217.183964  -4068.124621  4.416082  1.0\n","2     9.138451   4413.412028  0.425317  0.0\n","3   363.824242  15474.760647  1.094119  0.0\n","4  -768.812047  -7963.932192  1.870536  0.0"],"text/html":["\n","  <div id=\"df-21b4b899-a2f3-4c78-9105-091689d58e4f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>f1</th>\n","      <th>f2</th>\n","      <th>f3</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-195.871045</td>\n","      <td>-14843.084171</td>\n","      <td>5.532140</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-1217.183964</td>\n","      <td>-4068.124621</td>\n","      <td>4.416082</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9.138451</td>\n","      <td>4413.412028</td>\n","      <td>0.425317</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>363.824242</td>\n","      <td>15474.760647</td>\n","      <td>1.094119</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-768.812047</td>\n","      <td>-7963.932192</td>\n","      <td>1.870536</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21b4b899-a2f3-4c78-9105-091689d58e4f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-21b4b899-a2f3-4c78-9105-091689d58e4f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-21b4b899-a2f3-4c78-9105-091689d58e4f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"FI18joJ_kbhZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b00f530-cb31-4fc8-c827-4b375a8f2f6c","executionInfo":{"status":"ok","timestamp":1661870583075,"user_tz":-330,"elapsed":16,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}}},"source":["data.corr()['y']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["f1    0.067172\n","f2   -0.017944\n","f3    0.839060\n","y     1.000000\n","Name: y, dtype: float64"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"u40oCVMikbhc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"66751534-ea4e-42b2-9d98-68e1e966b8a3","executionInfo":{"status":"ok","timestamp":1661870590128,"user_tz":-330,"elapsed":461,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}}},"source":["data.std()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["f1      488.195035\n","f2    10403.417325\n","f3        2.926662\n","y         0.501255\n","dtype: float64"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"yQIbNaHskbhe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be9471d9-6200-4e11-b1fa-f5da8e5d6263","executionInfo":{"status":"ok","timestamp":1661870592554,"user_tz":-330,"elapsed":23,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}}},"source":["X=data[['f1','f2','f3']].values\n","Y=data['y'].values\n","print(X.shape)\n","print(Y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(200, 3)\n","(200,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"aUxp9-qEkbhh"},"source":["# What if our features are with different variance \n","\n","<pre>\n","* <b>As part of this task you will observe how linear models work in case of data having feautres with different variance</b>\n","* <b>from the output of the above cells you can observe that var(F2)>>var(F1)>>Var(F3)</b>\n","\n","> <b>Task1</b>:\n","    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' and check the feature importance\n","    2. Apply SVM(SGDClassifier with hinge) on 'data' and check the feature importance\n","\n","> <b>Task2</b>:\n","    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' after standardization \n","       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n","    2. Apply SVM(SGDClassifier with hinge) on 'data' after standardization \n","       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n","\n","</pre>"]},{"cell_type":"markdown","metadata":{"id":"TbMnsrxakbhi"},"source":["<h3><font color='blue'> Make sure you write the observations for each task, why a particular feature got more importance than others</font></h3>"]},{"cell_type":"markdown","source":["## 1. Task-1\n","\n","a) Logistic Regression (SGDClassifier with log loss)"],"metadata":{"id":"g4pnmjQSu0aP"}},{"cell_type":"code","source":["clf_1=SGDClassifier(loss='log') \n","clf_1.fit(X,Y)\n","print(\"Accuracy: \",clf_1.score(X,Y)) #Accuracy of the Model\n","\n","feature_importance=clf_1.coef_[0]\n","print('Feature Importance Coefficients')\n","for index, fi in enumerate(feature_importance):\n","  print(\"f{}= {}\".format(index+1,abs(feature_importance[index])))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QU3bvTksu6Tn","executionInfo":{"status":"ok","timestamp":1661870623309,"user_tz":-330,"elapsed":372,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}},"outputId":"19cd98e1-6e33-4d4b-b135-4cc91e68502a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.47\n","Feature Importance Coefficients\n","f1= 2888.3605521018426\n","f2= 1900.0711926308315\n","f3= 10267.57846259901\n"]}]},{"cell_type":"markdown","source":["\n","\n","b) SVM (SGDClassifier with Hinge Loss)\n"],"metadata":{"id":"0_TydOUrDlRu"}},{"cell_type":"code","source":["clf_2=SGDClassifier(loss='hinge') \n","clf_2.fit(X,Y)\n","print(\"Accuracy: \",clf_2.score(X,Y)) #Accuracy of the Model\n","\n","feature_importance=clf_2.coef_[0]\n","print('Feature Importance Coefficients')\n","for index, fi in enumerate(feature_importance):\n","  print(\"f{}= {}\".format(index+1,abs(feature_importance[index])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kF5VRCzNDVSS","executionInfo":{"status":"ok","timestamp":1661870731688,"user_tz":-330,"elapsed":365,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}},"outputId":"a0f58330-eef3-41be-ba5a-dd0ed4bcce02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.47\n","Feature Importance Coefficients\n","f1= 8898.536041326339\n","f2= 8425.3558028445\n","f3= 10234.290913221057\n"]}]},{"cell_type":"markdown","source":["##OBSERVATIONS\n","\n","* As we look at the Correlation of the features with 'y', we see that f3 has the highest correlation, followed by f1 and then f2.\n","* Also, f3 has the least variance, followed by f1 and then f2.\n","* Feature importance of f3 is the highest, followed by f1 and then f2 in both Logistic Regression and SVM.\n","\n","Hence we may conclude that those features having the highest correlation with the class label and also the least variance will be assigned the highest weight (feature importance)."],"metadata":{"id":"gBI8ih16921c"}},{"cell_type":"markdown","source":["## 2. Task-2\n","\n","a) Logistic Regression (SGDClassifier with Log Loss) after Column Standardization\n"],"metadata":{"id":"-KMgFy4E94-p"}},{"cell_type":"code","source":["std_scaler= StandardScaler()\n","std_scaler.fit_transform(X)\n","clf_4=SGDClassifier(loss='log')\n","clf_4.fit(X,Y)\n","print(\"Accuracy: \",clf_4.score(X,Y))\n","\n","feature_importance=clf_4.coef_[0]\n","print('Feature Importance Coefficients')\n","for index, fi in enumerate(feature_importance):\n","  print(\"f{}= {}\".format(index+1,abs(feature_importance[index])))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mw2e6mxX-a0b","executionInfo":{"status":"ok","timestamp":1661871051039,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}},"outputId":"4366023e-665e-4b38-ff73-4cf1e64d15d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.47\n","Feature Importance Coefficients\n","f1= 6573.532837143187\n","f2= 12847.575225646253\n","f3= 10608.854530679522\n"]}]},{"cell_type":"markdown","source":["b) SVM (SGDClassifier with hinge loss) after Column Standardization\n","\n","\n"],"metadata":{"id":"7TKX7thZD-Xe"}},{"cell_type":"code","source":["std_scaler= StandardScaler()\n","std_scaler.fit_transform(X)\n","clf_5=SGDClassifier(loss='hinge')\n","clf_5.fit(X,Y)\n","print(\"Accuracy: \",clf_5.score(X,Y))\n","\n","feature_importance=clf_5.coef_[0]\n","print('Feature Importance Coefficients')\n","for index, fi in enumerate(feature_importance):\n","  print(\"f{}= {}\".format(index+1,abs(feature_importance[index])))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNk5tB39ECl5","executionInfo":{"status":"ok","timestamp":1661871094915,"user_tz":-330,"elapsed":8,"user":{"displayName":"Aditya R","userId":"17681643623017385042"}},"outputId":"d53dc12b-de27-49f6-bd71-d815a2ec5dfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.525\n","Feature Importance Coefficients\n","f1= 578.1671125044395\n","f2= 4584.309589898638\n","f3= 9670.550611323984\n"]}]},{"cell_type":"markdown","source":["### OBSERVATION"],"metadata":{"id":"lYPTXxG3PJvT"}},{"cell_type":"markdown","source":["* Here we observe that column standardization has been done with to all 3 features. Hence now there would be no direct correlation with respect to the class label 'y' as all the features now have been scaled in the range 0 to 1.\n","\n","* Here we observe that f2 has the highest feature importance followed by f3 and then f1 in Logistic Regression.\n","\n","* In SVM, we find that f3 has the highest feature importance, followed by f2 and then f1. Also the accuracy of this model is the highest among all.\n","\n","Hence the overall conclusion is we have to Scale the features using Standard Scaler so that there is no direct collinearity between the features and the class labels based on which the feature weights will be assigned."],"metadata":{"id":"YRDW9jddSizv"}}]}